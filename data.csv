Title,url,Abstract
1. Augmenting Product Defect Surveillance Through Web Crawling and Machine Learning in Singapore.,https://scispace.com/papers/augmenting-product-defect-surveillance-through-web-crawling-4l4s621umh,"Abstract: Substandard medicines are medicines that fail to meet their quality standards and/or specifications. Substandard medicines can lead to serious safety issues affecting public health. With the increasing number of pharmaceuticals and the complexity of the pharmaceutical manufacturing supply chain, monitoring for substandard medicines via manual environmental scanning can be laborious and time consuming. A web crawler was developed to automatically detect and extract alerts on substandard medicines published on the Internet by regulatory agencies. The crawled data were labelled as related to substandard medicines or not. An expert-derived keyword-based classification algorithm was compared against machine learning algorithms to identify substandard medicine alerts on two validation datasets (n = 4920 and n = 2458) from a later time period than training data. Models were comparatively assessed for recall, precision and their F1 scores (harmonic mean of precision and recall). The web crawler routinely extracted alerts from the 46 web pages belonging to nine regulatory agencies. From October 2019 to May 2020, 12,156 unique alerts were crawled of which 7378 (60.7%) alerts were set aside for validation and contained 1160 substandard medicine alerts (15.7%). An ensemble approach of combining machine learning and keywords achieved the best recall (94% and 97%), precision (85% and 80%) and F1 scores (89% and 88%) on temporal validation. Combining robust web crawler programmes with rigorously tested filtering algorithms based on machine learning and keyword models can automate and expand horizon scanning capabilities for issues relating to substandard medicines."
2. Weakly supervised learning for an effective focused web crawler,https://scispace.com/papers/weakly-supervised-learning-for-an-effective-focused-web-572sfpwsyc,"Abstract: Focused crawler traverses the Web to only collect pages that are relevant to a particular topic, and is increasingly considered as a way to get around the scalability issues with current general-purpose search engines. But the data diversity in the Web forces these crawlers to face three significant problems: (i) inconsistency, (ii) ubiquity, and (iii) ambiguity, which causes misguidance in crawling. To handle these issues, this paper proposes a weakly supervised Gated Recurrent Unit (GRU) mechanism for an adaptive focused web crawler framework that matches semantically relevant topics and webpagecontent. This weakly supervised Gated Recurrent Unit model accepts the vector form of the topic and the fetched webpage as input to produce meaningful semantic vectors and incorporates the Manhattan distance rule to compute the topical relevance of the webpage. The proposed mechanism guides the focused crawler in downloading more relevant web pages by finding the relevant hyperlinks and omitting the irrelevant hyperlinks concerning the topic. The proposed method helps the focused crawler to semantically find, arrange, and index the web pages in a relatively narrow segment of the web to solve the inconsistency, ubiquity, and ambiguity problems of the focused crawlers. The experimental results indicate that the proposed technique outperforms the state−of−the−art approaches in terms of harvestrate, precision, recall, harmonicmean, and irrelevanceratio. In summary, the strategy described here works well and is important for focused crawlers."
3. An Automated Customizable Live Web Crawler for Curation of Comparative Pharmacokinetic Data: An Intelligent Compilation of Research-Based Comprehensive Article Repository,https://scispace.com/papers/an-automated-customizable-live-web-crawler-for-curation-of-wwsi4209,"Abstract: Data curation has significant research implications irrespective of application areas. As most curated studies rely on databases for data extraction, the availability of data resources is extremely important. Taking a perspective from pharmacology, extracted data contribute to improved drug treatment outcomes and well-being but with some challenges. Considering available pharmacology literature, it is necessary to review articles and other scientific documents carefully. A typical method of accessing articles on journal websites is through long-established searches. In addition to being labor-intensive, this conventional approach often leads to incomplete-content downloads. This paper presents a new methodology with user-friendly models to accept search keywords according to the investigators’ research fields for metadata and full-text articles. To accomplish this, scientifically published records on the pharmacokinetics of drugs were extracted from several sources using our navigating tool called the Web Crawler for Pharmacokinetics (WCPK). The results of metadata extraction provided 74,867 publications for four drug classes. Full-text extractions performed with WCPK revealed that the system is highly competent, extracting over 97% of records. This model helps establish keyword-based article repositories, contributing to comprehensive databases for article curation projects. This paper also explains the procedures adopted to build the proposed customizable-live WCPK, from system design and development to deployment phases."
4. Making Javascript Render Decisions to Optimize Security-Oriented Crawler Process,https://scispace.com/papers/making-javascript-render-decisions-to-optimize-security-ie6xn3x15mwy,"Abstract: The widespread use of web applications requires important changes in cybersecurity to protect online services and data. In the process of identifying security vulnerabilities in web applications, a systematic approach is employed to detect and mitigate cybersecurity risks. This approach utilizes web crawlers to identify attack vectors. Traditional web crawling methods are resource-intensive and often need to be more efficient in handling dynamic JavaScript-rich content. Addressing this crucial gap, our study introduces an innovative approach to predict the necessity of JavaScript rendering, thereby enhancing the effectiveness and efficiency of security-oriented web crawlers. This approach seeks to reduce computational requirements and quicken the security evaluation process through the use of machine learning algorithms. By utilizing a dataset containing the source code from the main pages of 17,160 websites, our experimental results demonstrate a 20% reduction in execution time compared to full JavaScript rendering, indicating an improvement in resource usage without any significant reduction in coverage. Our methodology significantly improves the efficiency of security-focused web crawlers and helps security scanners to detect security risks of web applications with fewer resources."
5. Establishing a framework for the ethical and legal use of web scrapers by cybercrime and cybersecurity researchers: learnings from a systematic review of Australian research,https://scispace.com/papers/establishing-a-framework-for-the-ethical-and-legal-use-of-skx4mph5z0,"Abstract: Abstract The Internet has become an increasingly attractive location for collecting data about cyber threats, driven by the abundance of quality data available and accessible online. As such, researchers and practitioners have turned to automated data collection technologies (ADCT), including ‘web crawlers’ and ‘web scrapers’, to study these threats. The rapid proliferation of ADCT has meant directions for their ethical and legal operation have been slow to adapt, with no clear guidelines regulating their use for research. This article identifies the relevant ethical and legal frameworks guiding the deployment of ADCT in Australia for cybersecurity research. This is accomplished through a systematic review of research within this context, coupled with ethical and jurisprudential analysis. We argue that the use of ADCT can be both ethical and legal, but only where mitigating measures are implemented. We provide a series of practical directions to guide researchers and practitioners when navigating this novel terrain."
6. Humkinar: Construction of a Large Scale Web Repository and Information System for Low Resource Urdu Language,https://scispace.com/papers/humkinar-construction-of-a-large-scale-web-repository-and-6dv9o7z9tytw,"Abstract: Online content availability, commercial viability, and technological advancements for English and European languages direct mainstream search engines to prioritize the search results of these high-resource languages. This makes it challenging for low-resource language users to access the search results in regional languages which is essential to promote literacy, inclusion, and digital accessibility. In this article, we create Humkinar– a Urdu language search engine using open-source tools. Our search engine is designed with five key components: computing infrastructure, data collector, search manager, web analytics engine, and user interface. First, our in-house computing infrastructure offers 160 GB RAM, 80 cores, and 30 TB memory to support the operations of the search engine. Next, we customize an open-source web crawler with a specialized Urdu language-focused URL selection algorithm, webpage parser, and content selection mechanism to collect Urdu webpages with optimized computing and Internet resources. We also employ specialized content scrapers to collect targeted and high-priority Urdu content like news articles, Wikipedia, poetry, and books. Overall, our data collector module has successfully curated a repository containing 14 million crawled webpages and 2.2 million scraped Urdu documents. Also, we design post-processing tools for tasks such as topic classification, de-duplication, profanity assessment, text summarization, and the scoring of website quality specific to the Urdu language. In addition, acknowledging the limitations of applying conventional ranking signals to Urdu language, search manager utilizes our seven derived ranking signals for search results. These signals are tuned to emphasize the richness and quality of Urdu language websites and content in search results. Moreover, we incorporate a web analytics engine into our search engine to collect and analyze user actions and metadata to enhance the overall functionality and effectiveness of the search engine. Our web analytics engine has recorded 400K user interactions from 83 countries conducted through the interactive user interface. Finally, we conduct usability testing of search engine with native Urdu language speakers to assess the strengths and weaknesses of our search engine."
7. Applying ontology learning and multi-objective ant colony optimization method for focused crawling to meteorological disasters domain knowledge,https://scispace.com/papers/applying-ontology-learning-and-multi-objective-ant-colony-110yhcxm,"Abstract: The focused crawler based on semantic analysis is a research hotspot in the field of information retrieval. The domain ontology is generally applied to construct the topic model of the focused crawler. In order to overcome the limitations of builders' knowledge reserve and subjective consciousness in the process of constructing artificially ontology, a semi-automatic construction method of domain ontology based on ontology learning technology combining the latent Dirichlet allocation and the Apriori algorithm is proposed in this article. When evaluating the relevance between a hyperlink and a specific topic, the joint evaluation method considering both the web text and the link structure is usually used. However, the traditional weighted sum method is difficult to reasonably determine the optimal weights of these evaluating indicators. To solve this problem, a multi-objective optimization model for link evaluation and a subsequent multi-objective ant colony optimization algorithm (MOACO) are proposed. In the MOACO, a method of the nearest farthest candidate solution (NFCS) is combined with the fast non-dominated sorting to select a set of Pareto-optimal hyperlinks and guide the crawlers’ search directions. The experimental results of the focused crawling on the domain knowledge of typhoon disasters and rainstorm disasters prove that the ability of the proposed focused crawlers to retrieve topic-relevant webpages."
8. Building a Korean Language Teaching Resource Library Based on Python Crawler,https://scispace.com/papers/building-a-korean-language-teaching-resource-library-based-2qu42x3udj,"Abstract: Abstract Nowadays, with the popularization of online education how to efficiently and accurately obtain the required educational resources from web pages is one of the key concerns in the teaching process. Based on the principle of maximum distance, the study improves the K-mean and establishes a learner group feature model based on the DM-K-mean clustering algorithm. In addition, for the problem that the importance of learning resources to learners changes with time, this paper integrates the time information into the neural collaborative filtering algorithm through the clustering classification algorithm. It proposes a deep learning-based recommendation algorithm for Korean language teaching resources. Python crawler technology is used to obtain relevant experimental data from the online teaching platform to verify the performance of the proposed model, so as to construct the Korean language teaching resource base. The learner group characteristic model classifies the sample students into three categories: excellent (0.489), good (0.307), and average (0.204) learning situations. The HR and NDCG values of the Improved NeuMF resource recommendation model have been improved by 3.6% and 2.2% compared to the NeuCF model, respectively, and the performance is optimal under various factors. The proposed system for teaching Korean language resources in this paper can recommend resources based on learner profiles to help learners access teaching resources and improve efficiency."
"9. The Internet Never Forgets: A Four-Step Scraping Tutorial, Codebase, and Database for Longitudinal Organizational Website Data",https://scispace.com/papers/the-internet-never-forgets-a-four-step-scraping-tutorial-3nnuukukl1qb,"Abstract: Websites represent a crucial avenue for organizations to reach customers, attract talent, and disseminate information to stakeholders. Despite their importance, strikingly little work in the domain of organization and management research has tapped into this source of longitudinal big data. In this paper, we highlight the unique nature and profound potential of longitudinal website data and present novel open-source code- and databases that make these data accessible. Specifically, our codebase offers a general-purpose setup, building on four central steps to scrape historical websites using the Wayback Machine. Our open-access CompuCrawl database was built using this four-step approach. It contains websites of North American firms in the Compustat database between 1996 and 2020—covering 11,277 firms with 86,303 firm/year observations and 1,617,675 webpages. We describe the coverage of our database and illustrate its use by applying word-embedding models to reveal the evolving meaning of the concept of “sustainability” over time. Finally, we outline several avenues for future research enabled by our step-by-step longitudinal web scraping approach and our CompuCrawl database."
10. Web crawling based context aware recommender system using optimized deep recurrent neural network,https://scispace.com/papers/web-crawling-based-context-aware-recommender-system-using-46rwoi14qr,"Abstract: Recommendation systems are obtaining more attention in various application fields especially e-commerce, social networks and tourism etc. The top items are recommended based on the ability of recommender system which predict the future preference out of the available items. Because of the internet, the people in the current society has too many options that’s why the recommendation system is very essential. The recommendation is achieved by the particular users who predict the ratings for numerous items and recommend those items to other users. Majorly, content and collaborative filtering techniques are employed in typical recommendation systems to find user preferences and provide final recommendations. But, these systems commonly lacks to take growing user preferences in various contextual factors. Context aware recommendation systems consider various contextual parameters into account and attempt to catch user preferences appropriately. The majority of the work in the recommender system domain focuses on increasing the recommendation accuracy by employing several proposed approaches where the main motive remains to maximize the accuracy of recommendations while ignoring other design objectives, such as a user’s an item’s context. Therefore, in this paper an effective deep learning based context aware recommendation model is proposed which can be act as an efficient recommender system by showing minimum error during recommendation. Initially, the dataset is pre-processed using Natural Language Tool Kit (NLTK) in Python platform. After pre-processing, the TF–IDF and word embedding model is used for every pre-processed reviews to extract the features and contextual information. The extracted feature is considered as an input of density based clustering to group the negative, neutral and positive sentiments of user reviews. Finally, deep recurrent neural Network (DRNN) is employed to get the most preferable user from every cluster. The recurrent neural network model parameter values are initialized through the fitness computation of Bald Eagle Search (BES) algorithm. The proposed model is implemented using NYC Restaurant Rich Dataset using Python programming platform and performance is evaluated based on the metrics of accuracy, precision, recall and compared with existing models. The proposed recommendation model achieves 99.6% accuracy which is comparatively higher than other machine learning models."
"11. Web Scraping for Migration, Mobility, and Migrant Integration Studies: Introduction, Application, and Potential Use Cases",https://scispace.com/papers/web-scraping-for-migration-mobility-and-migrant-integration-9vl47ot86r,"Abstract: Web scraping, a technique for extracting data from web pages, has been in use for decades, yet its utilization in the field of migration, mobility, and migrant integration studies has been limited. The field faces notorious limitations regarding data access and availability, particularly in low-income settings. Web scraping has the potential to provide new datasets for further qualitative and quantitative analysis. Web scraping requires no financial resources, is agnostic to epistemic divides in the field, reduces researcher bias, and increases transparency and replicability of data collection. As large providers of digital data such as Facebook or Twitter increasingly restrict access to their data for researchers, web scraping will become more important in the future and deserves its place in the toolbox of migration and mobility scholars. This short and nontechnical methods note introduces the fundamental concepts of web scraping, provides guidance on how to learn the technique, showcases practical applications of web scraping in the study of migrant populations, and discusses potential future use cases."
12. How to harness the power of web scraping for medical and surgical research: An application in estimating international collaboration.,https://scispace.com/papers/how-to-harness-the-power-of-web-scraping-for-medical-and-foh37pt1w1,"Abstract: The transformative potential of web scraping in surgical research through a comprehensive analysis of its revolutionary applications and profound impact is now within reach. This manuscript unveils the pivotal role of web scraping in driving innovation, enabling more effective management of human capital dynamics, and enhancing patient outcomes in the surgical field. As an example, we demonstrate how web scraping can uncover insights into international collaboration in surgery research revealing limited collaboration between surgeons in developed and developing countries."
13. Big Enough to Care Not Enough to Scare! Crawling to Attack Recommender Systems,https://scispace.com/papers/big-enough-to-care-not-enough-to-scare-crawling-to-attack-2opb3f9h1z,"Abstract: Online recommendation services, such as e-commerce sites, rely on a vast amount of knowledge about users/items that represent an invaluable resource. Part of this acquired knowledge is public and can be accessed by anyone through the Internet. Unfortunately, that same knowledge can be used by competitors or malicious users. A large body of research proposes methods to attack recommender systems, but most of these works assume that the attacker knows or can easily access the rating matrix. In practice, this information is not directly accessible, but can only be gathered via crawling."
14. Raspando la Arqueología: Una Aproximación Metodológica desde el Web Scraping y Text Mining,https://scispace.com/papers/raspando-la-arqueologia-una-aproximacion-metodologica-desde-er89yirqnh,"Abstract: A medida que la cantidad de información disponible en la web aumenta, también lo hace la tarea de localizarla y analizarla, por lo cual realizar esta tarea de forma manual puede ser costosa en función al tiempo y esfuerzo invertido. Aunque los buscadores y los motores de bases de datos pueden ayudar a encontrar la información requerida, en infraestructuras digitales grandes donde los resultados de búsqueda se cuentan por millares – o más– se precisan de nuevas herramientas para obtener el contenido buscado de manera efectiva. Este trabajo propone la aplicación de Web Scraping y Text Mining como insumos metodológicos para poder compilar y procesar grandes volúmenes de datos en infraestructuras digitales de una forma más automatizada. La automatización de ambos procesos aporta una gran ventaja al analizar corpus textuales de miles de registros lo cual simplifica de manera significativa la obtención de diferentes tipos de datos, facilitando el trabajo considerablemente. Se espera que esta contribución permita ampliar las posibilidades de la comunidad arqueológica en clave de una metodología novedosa para la obtención y el manejo de datos estructurados y no estructurados que pueden ser integrados a las investigaciones de la comunidad arqueológica en general."
15. CrawlPhish: Large-scale Analysis of Client-side Cloaking Techniques in Phishing,https://scispace.com/papers/crawlphish-large-scale-analysis-of-client-side-cloaking-1sr6ysszod,"Abstract: Phishing is a critical threat to Internet users. Although an extensive ecosystem serves to protect users, phishing websites are growing in sophistication, and they can slip past the ecosystem’s detection systems—and subsequently cause real-world damage—with the help of evasion techniques. Sophisticated client-side evasion techniques, known as cloaking, leverage JavaScript to enable complex interactions between potential victims and the phishing website, and can thus be particularly effective in slowing or entirely preventing automated mitigations. Yet, neither the prevalence nor the impact of client-side cloaking has been studied.In this paper, we present CrawlPhish, a framework for automatically detecting and categorizing client-side cloaking used by known phishing websites. We deploy CrawlPhish over 14 months between 2018 and 2019 to collect and thoroughly analyze a dataset of 112,005 phishing websites in the wild. By adapting state-of-the-art static and dynamic code analysis, we find that 35,067 of these websites have 1,128 distinct implementations of client-side cloaking techniques. Moreover, we find that attackers’ use of cloaking grew from 23.32% initially to 33.70% by the end of our data collection period. Detection of cloaking by our framework exhibited low false-positive and false-negative rates of 1.45% and 1.75%, respectively. We analyze the semantics of the techniques we detected and propose a taxonomy of eight types of evasion across three high-level categories: User Interaction, Fingerprinting, and Bot Behavior.Using 150 artificial phishing websites, we empirically show that each category of evasion technique is effective in avoiding browser-based phishing detection (a key ecosystem defense). Additionally, through a user study, we verify that the techniques generally do not discourage victim visits. Therefore, we propose ways in which our methodology can be used to not only improve the ecosystem’s ability to mitigate phishing websites with client-side cloaking, but also continuously identify emerging cloaking techniques as they are launched by attackers."
16. Web Connector: A Unified API Wrapper to Simplify Web Data Collection,https://scispace.com/papers/web-connector-a-unified-api-wrapper-to-simplify-web-data-2b3glaznfx,"Abstract: Collecting structured data from Web APIs, such as the Twitter API, Yelp Fusion API, Spotify API"
17. Proactive Institutional Repository Collection Development Techniques: Archiving Gold Open Access Articles and Metadata Retrieved with Web Scraping,https://scispace.com/papers/proactive-institutional-repository-collection-development-2t40rz4ayv,"Abstract: Abstract Many institutions face low deposit rates with their institutional repositories despite investing substantial resources in implementing and supporting these systems. Deposit rates are higher in IRs that offer mediated deposits; however, this can be a time and labor intensive process. This article describes a method for copying open access articles and corresponding descriptive metadata from open repositories for archiving in an institutional repository using Beautiful Soup and Selenium as web scraping tools. This method quickly added hundreds of articles to an IR without relying on faculty participation or consulting publisher policies, increasing repository downloads and usage."
18. D <scp>e</scp> ME <scp>t</scp> RIS: Counting (near)-Cliques by Crawling,https://scispace.com/papers/d-scp-e-scp-me-scp-t-scp-ris-counting-near-cliques-by-33rizy17t4u9,"Abstract: We study the problem of approximately counting cliques and near cliques in a graph, where the access to the graph is only available through crawling its vertices. This model has been introduced recently to capture real-life scenarios in which the entire graph is too massive to be stored as a whole or be scanned entirely. Sampling vertices independently is non-trivial in this model, thus algorithms which rely on sampling often use a random walk. The goal is to provide an accurate estimate by seeing only a small portion of the graph. This model is known as the random walk model or the neighborhood query model. We introduce D e ME t RIS: Dense Motif Estimation through Random Incident Sampling. This method provides a scalable algorithm for clique and near clique counting in the random walk model. We prove the correctness of our algorithm through rigorous mathematical analysis and extensive experiments. Both our theoretical results and our experiments show that D e ME t RIS obtains a high precision estimation by only crawling a sub-linear portion on vertices. Therefore, we demonstrate a significant improvement over previous known results."
"19. Web-scraping applied to acquire difficult to access animal disease outbreak information, using African Swine Fever in Europe as an example",https://scispace.com/papers/web-scraping-applied-to-acquire-difficult-to-access-animal-4k5wnpq2r1,"Abstract: Surveillance data are key to informing decisions on the control and prevention of transboundary and emerging diseases. Here, we describe new methods for acquiring difficult to access, publicly available disease surveillance data. We use World Organisation for Animal Heath (OIE) data on African Swine Fever (ASF) outbreaks in European countries to showcase the importance of adequate disease surveillance data to inform decision-making. The data acquired using these methods allow for large scale, geospatial outbreak mapping and estimation of summary statistics for any listed terrestrial disease in the OIE World Animal Health Information System (WAHIS) database. These techniques will make valuable epidemiological data more accessible to the scientific community, aiding further insight into the occurrence and spread of transboundary and emerging diseases in a timely manner, fulfilling an important function of disease surveillance."
20. DeMEtRIS: Counting (near)-Cliques by Crawling,https://scispace.com/papers/demetris-counting-near-cliques-by-crawling-1pyj2wgo,"Abstract: We study the problem of approximately counting cliques and near cliques in a graph, where the access to the graph is only available through crawling its vertices; thus typically seeing only a small portion of it. This model, known as the random walk model or the neighborhood query model has been introduced recently and captures real-life scenarios in which the entire graph is too massive to be stored as a whole or be scanned entirely and sampling vertices independently is non-trivial in it. We introduce DeMEtRIS: Dense Motif Estimation through Random Incident Sampling. This method provides a scalable algorithm for clique and near clique counting in the random walk model. We prove the correctness of our algorithm through rigorous mathematical analysis and extensive experiments. Both our theoretical results and our experiments show that DeMEtRIS obtains a high precision estimation by only crawling a sub-linear portion on vertices, thus we demonstrate a significant improvement over previously known results."
21. Tanium reveal: a federated search engine for querying unstructured file data on large enterprise networks,https://scispace.com/papers/tanium-reveal-a-federated-search-engine-for-querying-115dx40mmf,"Abstract: Tanium Reveal is a federated search engine deployed on large-scale enterprise networks that is capable of executing data queries across billions of private data files within 60 seconds. Data resides at the edge of networks, potentially distributed on hundreds of thousands of endpoints. The anatomy of the search engine consists of local inverse indexes on each endpoint and a global communication platform called Tanium for issuing search queries to all endpoints. Reveal enables asynchronous parsing and indexing on endpoints without noticeable impact to the endpoints’ primary functionality. The engine harnesses the Tanium platform, which is based on a self-organizing, fault-tolerant, scalable, linear chain communication scheme. We demonstrate a multi-tier workflow for executing search queries across a network and for viewing matching snippets of text on any endpoint. We analyze metrics for federated indexing and searching in multiple environments including a production network with 1 . 05 billion searchable files distributed across 4236 endpoints. While primarily focusing on Boolean, phrase, and similarity query types, Reveal is compatible with further automation (e.g., semantic classification based on machine learning). Lastly, we discuss safeguards for sensitive information within Reveal including cryptographic hashing of private text and role-based access control (RBAC)."
22. Swarm optimized cluster based framework for information retrieval,https://scispace.com/papers/swarm-optimized-cluster-based-framework-for-information-smx6q9jnrc,"Abstract: This work explores the integrated power of swarm intelligence and advances in data mining techniques to solve the information retrieval (IR) problem of rapidly growing digital content on the World Wide Web. We propose a swarm optimized cluster based framework with frequent pattern mining techniques to retrieve user-specific knowledge from extensive document collections. In the pre-processing phase, we split the task into two sub-tasks. The first is to decompose the document collection into groups using a bio-inspired K-Flock clustering algorithm, while the second extracts frequent patterns from each cluster using a memory-efficient Recursive Elimination (RElim) algorithm. In the next phase, we implement a cosine similarity based probabilistic model to retrieve query-specific documents from clusters based on the matching scores between the closed frequent patterns of queries and clusters. The performance of a system is evaluated by conducting several experiments which are carried out on five well-known, diverse and variable size datasets viz- TREC 2014-15 CDS (Clinical Decision Support) datasets containing 733,138 records, OHSUMED dataset with 348,566 records from Medline database, NPL dataset with 11,429 records, LISA document collection of 6004 records, CACM (Collection of ACM) dataset of 3204 records. The results show that the proposed IR framework significantly outperforms the traditional sequential IR approach and other state-of-the-art IR approaches, both in terms of the quality of the returned documents and the time of execution."
23. A Big Data architecture for early identification and categorization of dark web sites,https://scispace.com/papers/a-big-data-architecture-for-early-identification-and-2u0a3d4f9m,"Abstract: The dark web has become notorious for its association with illicit activities and there is a growing need for systems to automate the monitoring of this space. This paper proposes an end-to-end scalable architecture for the early identification of new Tor sites and the daily analysis of their content. The solution is built using an Open Source Big Data stack for data serving with Kubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion addresses in different sources (threat intelligence, code repositories, web-Tor gateways, and Tor repositories), downloading the HTML from Tor and deduplicating the content using MinHash LSH, and categorizing with the BERTopic modeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days, the system identified 80,049 onion services and characterized 90% of them, addressing the challenge of Tor volatility. A disproportionate amount of repeated content is found, with only 6.1% unique sites. From the HTML files of the dark sites, 31 different low-topics are extracted, manually labeled, and grouped into 11 high-level topics. The five most popular included sexual and violent content, repositories, search engines, carding, cryptocurrencies, and marketplaces. During the experiments, we identified 14 sites with 13,946 clones that shared a suspiciously similar mirroring rate per day, suggesting an extensive common phishing network. Among the related works, this study is the most representative characterization of onion services based on topics to date."
24. The use of spider webs in the monitoring of air quality—A review,https://scispace.com/papers/the-use-of-spider-webs-in-the-monitoring-of-air-quality-a-2kcjigwr,"Abstract: Methods for using spider webs as passive air samplers have been developed over recent years and reported in more than a dozen articles. In this article, we present the typical procedures followed when using this new tool and critically review its application in air pollution assessment. To understand the state of research and application of spider webs in this field, we describe some advantages and disadvantages of their use in the analyses of air contaminants. The aim is to summarize the current knowledge on this subject, highlight gaps in the present studies, and arouse the interest of scientists on this issue. The increased effort could result in the standardization of the method at the national and international level. Integr Environ Assess Manag 2023;19:32–44. © 2022 SETAC"
25. Web of venom: exploration of big data resources in animal toxin research,https://scispace.com/papers/web-of-venom-exploration-of-big-data-resources-in-animal-2szrpcnesfxb,"Abstract: Abstract Research on animal venoms and their components spans multiple disciplines, including biology, biochemistry, bioinformatics, pharmacology, medicine, and more. Manipulating and analyzing the diverse array of data required for venom research can be challenging, and relevant tools and resources are often dispersed across different online platforms, making them less accessible to nonexperts. In this article, we address the multifaceted needs of the scientific community involved in venom and toxin-related research by identifying and discussing web resources, databases, and tools commonly used in this field. We have compiled these resources into a comprehensive table available on the VenomZone website (https://venomzone.expasy.org/10897). Furthermore, we highlight the challenges currently faced by researchers in accessing and using these resources and emphasize the importance of community-driven interdisciplinary approaches. We conclude by underscoring the significance of enhancing standards, promoting interoperability, and encouraging data and method sharing within the venom research community."
"26. Design, modeling, and experimental analysis of the Crawler Unit for inspection in constrained space",https://scispace.com/papers/design-modeling-and-experimental-analysis-of-the-crawler-oseefr8cc6,"Abstract: Inspections of industrial and civil infrastructures prevent unexpected failures that may lead to loss of life. Although inspection robotics is gaining momentum, most of field operations are still performed by human workers. For inspection robots, the main limiting factors are the low versatility and reliability in dynamic, non-structured and highly complex environments. To tackle these issues, we have designed a modular and self-reconfigurable hybrid platform, which consists of three units: the mobile Main Base and two Crawler Units with docking interfaces. The Crawler Unit operates in constrained environments and narrow spaces, while the Main Base will inspect wide areas and deploy/recover the Crawler Units near/from inspection sites, as in marsupial robots. Docking interfaces will allow the Crawler Units to reconfigure into a snake robot or mobile manipulators. In particular, the Crawler Units consist of four modules connected by three kinematic chains for nine active joints in total. Each module is equipped with half active, half passive tracks for moving. This paper discusses in detail the dynamic model of the Crawler Unit, especially focusing on the definition of effective constraint equations, which closely model the system features avoiding common simplifications. Numerical simulations and physical experiments validate the proposed dynamic model of the Crawler Unit."
27. Spider monkey optimization based resource allocation and scheduling in fog computing environment,https://scispace.com/papers/spider-monkey-optimization-based-resource-allocation-and-22m047658v,"Abstract: Spider Monkey optimization (SMO) is a quite popular and recent swarm intelligence algorithm for numerical optimization. SMO is Fission-Fusion social structure based algorithm inspired by spider monkey’s behavior. The algorithm proves to be very efficient in solving various constrained and unconstrained optimization problems. This paper presents the application of SMO in fog computing. We propose a heuristic initialization based spider monkey optimization algorithm for resource allocation and scheduling in a fog computing network. The algorithm minimizes the total cost (service time and monetary cost) of tasks by choosing the optimal fog nodes. LJFP (longest job fastest processor), SJFP (shortest job fastest processor), and MCT (minimum completion time) based initialization of SMO are proposed and compared with each other. The performance is compared based on the parameters of average cost, average service time, average monetary cost, and the average cost per schedule. The results demonstrate the efficacy of MCT-SMO as compared to other heuristic initialization based SMO algorithms and PSO (Particle Swarm Optimization)."
"28. Dragonfly algorithm: a comprehensive survey of its results, variants, and applications",https://scispace.com/papers/dragonfly-algorithm-a-comprehensive-survey-of-its-results-8v24avphc1,"Abstract: This paper thoroughly introduces a comprehensive review of the so-called Dragonfly algorithm (DA) and highlights its main characteristics. DA is considered one of the promising swarm optimization algorithms because it successfully applied in a wide range of optimization problems in several fields, such as engineering design, medical applications, image processing, power and energy systems, and economic load dispatch problems. The review describes the available literature on DA, including its variants like binary, discrete, modify, and hybridization of DA. Conclusions focus on the current work on DA, highlighting its disadvantages with suggests possible future research directions. Researchers and practitioners of DA belonging to a wide range of audiences from the domains of optimization, engineering, medical, data mining, and clustering, among others will benefit from this study."
29. WebUltron: An Ultimate Retriever on Webpages Under the Model-Centric Paradigm,https://scispace.com/papers/webultron-an-ultimate-retriever-on-webpages-under-the-model-3brdbx9rc4,"Abstract: Document retrieval has been extensively studied within the index-retrieve framework for decades, which has withstood the test of time. However, this approach inherently segregates the indexing and retrieval processes, preventing a cohesive, end-to-end optimization. To bridge this divide, we introduce WebUltron, a revolutionary model-centric indexer for document retrieval. This system embeds the entirety of document knowledge within the model, striving for seamless end-to-end retrieval. Two primary challenges with this indexer are the representation of document identifiers (docids) and the model's training. Current methods grapple with docids that lack semantic depth and the constraints of limited supervised data, making scaling up to larger datasets challenging. Addressing this, we’ve engineered two novel docid types imbued with richer semantics that also streamline model inference. Further enhancing WebUltron's capabilities, we’ve developed a three-stage training regimen, leveraging deeper corpus insights and fortifying query-docid relationships. Experiments on two public datasets demonstrate the superiority of WebUltron over advanced baselines for document retrieval."
30. Enhancing SEO in Single-Page Web Applications in Contrast With Multi-Page Applications,https://scispace.com/papers/enhancing-seo-in-single-page-web-applications-in-contrast-3bqf5aqrsr,"Abstract: This paper comprehensively reviews methods for improving single-page applications’ visibility (SPAs) and user experience, focusing on the intricacies of search engine optimisation (SEO). This research contrasts the complexities and challenges in optimising SEO in SPAs instead of conventional multi-page applications (MPAs). It identifies vital optimisation methods and evaluates their applicability in the contemporary web landscape. The research method involves implementing the explored optimisation techniques across three distinct projects utilising emerging technologies for SPA, MPA, and a hybrid approach using Isomorphic JavaScript. These applications are systematically examined and subjected to a comparative analysis to assess the effectiveness of the optimisation strategies before and after applying the optimisation strategies. The empirical results substantiate that adopting an innovative approach to Client-Side rendering for the initial page load, combined with traditional SEO practices, performance enhancements, and tailored methodologies for specific technologies, facilitates SEO optimisation in SPAs at a level commensurate with MPAs. The findings of this work hold significant implications for web developers, offering insights and actionable strategies to augment visibility and performance in search engine results. By bridging the theoretical understanding with hands-on application and empirical analysis, the research contributes to the evolving field of web application development. It underscores the critical role of SEO optimisation in the context of SPAs, highlighting its importance for search engine rankings and overall user engagement and satisfaction. Code is available on GitHub:https://github.com/karolinakowalczyk?tab=repositories&q=TravelBLog"
31. The Legality and Ethics of Web Scraping in Archaeology,https://scispace.com/papers/the-legality-and-ethics-of-web-scraping-in-archaeology-2px8td5gxs,"Abstract:  Web scraping, the practice of automating the collection of data from websites, is a key part of how the internet functions, and it is an increasingly important part of the research tool kit for scientists, cultural resources professionals, and journalists. There are few resources intended to train archaeologists in how to develop web scrapers. Perhaps more importantly, there are also few resources that outline the normative, ethical, and legal frameworks within which scraping of archaeological data is situated. This article is intended to introduce archaeologists to web scraping as a research method, as well as to outline the norms concerning scraping that have evolved since the 1990s, and the current state of US legal frameworks that touch on the practice. These norms and legal frameworks continue to evolve, representing an opportunity for archaeologists to become more involved in how scraping is practiced and how it should be regulated in the future."
32. COVID-Scraper: An Open-Source Toolset for Automatically Scraping and Processing Global Multi-Scale Spatiotemporal COVID-19 Records,https://scispace.com/papers/covid-scraper-an-open-source-toolset-for-automatically-160mjcil38,"Abstract: In 2019, COVID-19 quickly spread across the world, infecting billions of people and disrupting the normal lives of citizens in every country. Governments, organizations, and research institutions all over the world are dedicating vast resources to research effective strategies to fight this rapidly propagating virus. With virus testing, most countries publish the number of confirmed cases, dead cases, recovered cases, and locations routinely through various channels and forms. This important data source has enabled researchers worldwide to perform different COVID-19 scientific studies, such as modeling this virus’s spreading patterns, developing prevention strategies, and studying the impact of COVID-19 on other aspects of society. However, one major challenge is that there is no standardized, updated, and high-quality data product that covers COVID-19 cases data internationally. This is because different countries may publish their data in unique channels, formats, and time intervals, which hinders researchers from fetching necessary COVID-19 datasets effectively, especially for fine-scale studies. Although existing solutions such as John’s Hopkins COVID-19 Dashboard and 1point3acres COVID-19 tracker are widely used, it is difficult for users to access their original dataset and customize those data to meet specific requirements in categories, data structure, and data source selection. To address this challenge, we developed a toolset using cloud-based web scraping to extract, refine, unify, and store COVID-19 cases data at multiple scales for all available countries around the world automatically. The toolset then publishes the data for public access in an effective manner, which could offer users a real time COVID-19 dynamic dataset with a global view. Two case studies are presented about how to utilize the datasets. This toolset can also be easily extended to fulfill other purposes with its open-source nature."
33. Current engagement with unreliable sites from web search driven by navigational search,https://scispace.com/papers/current-engagement-with-unreliable-sites-from-web-search-33bfbdwxrdyz,"Abstract: Do search engine algorithms systematically expose users to content from unreliable sites? There is widespread concern that they do, but little systematic evidence that search engine algorithms, rather than user-expressed preferences, are driving current exposure to and engagement with unreliable information sources. Using two datasets totaling roughly 14 billion search engine result pages (SERPs) from Bing, the second most popular search engine in the U.S., we show that search exposes users to few unreliable information sources. The vast majority of engagement with unreliable information sources from search occurs when users are explicitly searching for information from those sites, despite those searches being an extremely small share of the overall search volume. Our findings highlight the importance of accounting for user preference when examining engagement with unreliable sources from web search."
34. Predicting Trending Elements on Web Pages Using Machine Learning,https://scispace.com/papers/predicting-trending-elements-on-web-pages-using-machine-4rtg4eu5fu,"Abstract: AbstractEye-tracking data can be used to understand how users interact with web pages. Understanding the eye-movement sequences of multiple users is a challenging task because the sequence followed by each user tends to be different. Scanpath Trend Analysis (STA) brings multiple individual eye-movement sequences together and identifies a representative sequence as a trending path. However, eye-tracking data on a web page is required to determine the trending path. Our aim here is to investigate whether we can train Machine Learning (ML) algorithms to identify trending elements on a web page without collecting eye-tracking data on that web page. This article presents our experiments with different ML classification algorithms towards achieving that goal. To validate the experiments, we used two datasets from previous research, the first one included browsing and searching tasks and the second one included browsing and synthesis tasks. Our experiments show that the k-nearest neighbors algorithm (KNN) model can successfully identify the trending elements in the first dataset for both browsing (F1=≈91%) and searching tasks (F1=≈88%). However, the second dataset’s synthesis task results were not as successful as its browsing task results. Our work here shows that a model can be created to predict the trending elements in web pages solely with web page features but the task is a critical factor in the success of prediction.Keywords: Eye-trackingtrending pathscanpath trend analysismachine learning Disclosure statementNo potential conflict of interest was reported by the author(s).Notes1 Throughout this paper, we use AOI/segments interchangeably with visual elements of web pages.2 https://developers.google.com/web/tools/puppeteerAdditional informationNotes on contributorsNaziha Shekh KhalilNaziha Shekh Khalil holds BSc and MSc degrees in Computer Engineering from Middle East Technical University Northern Cyprus. Her research focuses on enhancing user experiences in the digital world through human-computer systems.Sukru EraslanSukru Eraslan is a lecturer at Middle East Technical University Northern Cyprus Campus. He completed his PhD in Computer Science at the University of Manchester. He then worked as a research associate at METU NCC and the University of Manchester. His primary research interests are centred around human–computer interaction. https://users.metu.edu.tr/seraslan/index.htmlYeliz YesiladaYeliz Yesilada earned her PhD in Computer Science from the University of Manchester in the UK. She is a Professor at Middle East Technical University Northern Cyprus Campus. Her primary research interest is the human–computer interaction, particularly user behaviour analysis and modelling, the mobile Web, and eye-tracking. https://www.yelizyesilada.info."
35. Search engine Performance optimization: methods and techniques,https://scispace.com/papers/search-engine-performance-optimization-methods-and-3o3t2ewc4u,"Abstract: <ns3:p>Background With the rapid advancement of information technology, search engine optimisation (SEO) has become crucial for enhancing the visibility and relevance of online content. In this context, the use of cloud platforms like Microsoft Azure is being explored to bolster SEO capabilities. Methods This scientific article offers an in-depth study of search engine optimisation. It explores the different methods and techniques used to improve the performance and efficiency of a search engine, focusing on key aspects such as result relevance, search speed and user experience. The article also presents case studies and concrete examples to illustrate the practical application of optimisation techniques. Results The results demonstrate the importance of optimisation in delivering high quality search results and meeting the increasing demands of users. Conclusions The article addresses the enhancement of search engines through the Microsoft Azure infrastructure and its associated components. It highlights methods such as indexing, semantic analysis, parallel searches, and caching to strengthen the relevance of results, speed up searches, and optimise the user experience. Following the application of these methods, a marked improvement was observed in these areas, thereby showcasing the capability of Microsoft Azure in enhancing search engines. The study sheds light on the implementation and analysis of these Azure-focused techniques, introduces a methodology for assessing their efficacy, and details the specific benefits of each method. Looking forward, the article suggests integrating artificial intelligence to elevate the relevance of results, venturing into other cloud infrastructures to boost performance, and evaluating these methods in specific scenarios, such as multimedia information search. In summary, with Microsoft Azure, the enhancement of search engines appears promising, with increased relevance and a heightened user experience in a rapidly evolving sector.</ns3:p>"
36. A modified WASPAS Method for the Evaluation of E- Commerce Websites Based on Pythagorean Fuzzy Information,https://scispace.com/papers/a-modified-waspas-method-for-the-evaluation-of-e-commerce-433jr0plocno,"Abstract: Web and business e-commerce site assessment is the act of comparing e-commerce sites based on their functionality, efficiency, or ability to meet the needs of a business or its clients. They are essential to keep up with the market changes and shift to provide better compliance with security requirements, and to optimize the interface to hold on to user attention. The modified versions of WASPAS method hybrid with Pythagorean fuzzy sets is exploited to perform an evaluation on the e-commerce websites like Amazon review dataset, Chnsenti Corp. and IMDB dataset using various performance indicators. The WASPAS model with balancing factor β = exp( <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</i> ) demonstrates an average accuracy of 94.01% for Amazon review dataset, 98.19% for Chnsenti Corp. dataset and 98.83% in the case of IMBD dataset with a mean execution time of 806.60 sec, 787.10 sec and 809.19 sec, respectively. The proposed model is compared with reported state of art results methods SVM, SVM hybrid with unigram, SVM hybrid with unigram, SVM hybrid with unigram & grid search techniques, XGBoost and ERF-XGB methods with their accuracy of 50.40%, 80.81%, 80.81%, 90.1 and 98.2, respectively. The global fitness value and mean execution time from 50 independent runs is calculated for these datasets and perform statistical indicators to see the stability of the study. The proposed approach significantly enhanced the performance and reduced the computational complexity, making it a promising tool for real-time detection and prediction in efficiency of employees in various organizations as a tool of growth."
37. A Novel Web Attack Detection Mechanism Using Maximal-Munch With Torrent Deep Network,https://scispace.com/papers/a-novel-web-attack-detection-mechanism-using-maximal-munch-14n3q67btd,"Abstract: A web attack is a harmful and deliberate attempt made by one person or group to gain access to another person's or group's data collection. Due to the incompatibility of the training algorithm for the Cross-Site Scripting (XSS) detection technique and the heterogeneity of attack load, the website was more frequently impacted by the detection of SQL injection attacks. Also, the language of the online sites has a significant impact on how well the current phishing detection system works, which is still a difficult issue. To address these problems, a novel Praise-Worthy Authentication technique is proposed which accurately detects phishing websites by checking the webpage's conformance using the hyperlink property. Also, a Maximal-Munch Algorithm-based ANN is proposed to prevent XSS attacks. The URLs associated with each webpage that is dragged will be sorted out to acquire URL parameters, and text patterns are matched at regular intervals to detect the XSS attack. This work also employs a Torrent Deep network with weight-bolster Algorithm to identify SQL injection by hackers, preventing significant network damage that would otherwise cause data leaks and website paralysis. This proposed Web-strafe Detection Framework has considerably increased the security of websites by identifying numerous threats."
38. A New Website Fingerprinting Method for Tor Hidden Service,https://scispace.com/papers/a-new-website-fingerprinting-method-for-tor-hidden-service-439age6nhexg,"Abstract: Although anonymous communication systems protect user privacy, they also facilitate evasion of network censorship. Currently, evaders use anonymous hidden services to carry out various illegal activities, which pose a serious threat to network management. To address this problem, a new website fingerprinting method that combines the Bidirectional Encoder Representations from Transformers (BERT) model and a Long Short-Term Memory (LSTM) network was proposed to improve the accuracy of website fingerprinting for Tor hidden services. The proposed method uses the BERT model to extract the semantic features of webpage content, and the LSTM model is combined to capture the long-term dependencies to achieve efficient recognition of website fingerprints. This method not only deals with the homepage, but also considers the features of sub-pages in depth, which can effectively improve the performance of the model in closed-world and open-world scenarios through deep textual and time-series feature learning. The experimental results show that compared with existing state-of-the-art techniques, the proposed method exhibits better performance in terms of key performance metrics."
39. Applying information mining technology in online entrepreneurship training course,https://scispace.com/papers/applying-information-mining-technology-in-online-5pkclmqawr0p,"Abstract: The purpose of this study is to deeply explore the application of information mining technology in online entrepreneurship training courses, and to improve students' learning effects and entrepreneurship success rate through systematic analysis and optimization of key data in the teaching process. With the development of online education, how to effectively use big data technology to meet personalized learning needs has become an important issue. This study takes several online entrepreneurship training courses as the research object, and uses information mining technology to extract and analyze students' behavioral data during course participation, including data on study time, interaction frequency, assessment results, etc. Through machine learning algorithms and association rule mining, the research revealed the main factors that affect students' learning effects and entrepreneurial success, and designed targeted teaching strategies, such as dynamically adjusting learning content, providing personalized feedback, optimizing learning paths, etc. Experimental results show that online courses using information mining technology significantly improve students' knowledge mastery and entrepreneurial success rate, especially in terms of personalized learning experience and teaching efficiency. In addition, this study also explores the application prospects of information mining technology in future online education. It is believed that through the combination with artificial intelligence (AI) technology, the intelligence and adaptability of online courses can be further enhanced to meet more diversified learning needs."
40. Information retrieval algorithms and neural ranking models to detect previously fact-checked information,https://scispace.com/papers/information-retrieval-algorithms-and-neural-ranking-models-4ir45jx65t,"Abstract: Although in the last decade several fact-checking organizations have emerged to verify misinformation, fake news has continued to proliferate, especially through social media platforms. Even though adopting improved detection strategies is of utmost importance, the fact-checking process could be optimised by verifying whether a claim has been previously fact-checked. Despite some ad-hoc information retrieval approaches having been recently proposed, the utility of modern (neural) retrieval systems have not been investigated yet. In this paper, we consider the standard two-phases retriever-reranker architecture and benchmark different state-of-the-art techniques from the information retrieval and Q&A literature. We design several experiments on a real-world Twitter dataset to analyse the efficiency and the effectiveness of the benchmark approaches. Our results show that combining standard and neural approaches is the most promising research direction to improve retrievers performance and that complex (neural) rerankers might still be efficient in practice since there is no need to process a high number of documents to improve ranking performance."
